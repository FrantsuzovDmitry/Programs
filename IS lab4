import numpy as np
def sigm(t):                        #сигмоида
  return 1 / (1 + np.exp(-t))

def sigm_der(t):                    #производная сигмоиды
  return sigm(t) * (1 - sigm(t))

def y_to_vector(y):                 #перевод ответа в бинарный вид
  y_full = np.zeros((len(y), 3))
  for j, yj in enumerate(y): 
    y_full[j, yj - 1] = 1
  return y_full
  
x = np.array([[1, 1, 0], [1, 0, 1], [0, 1, 1], [0, 1, 0], [0, 1, 1], [0, 0, 1], [0, 1, 0], [1, 1, 1], [0, 0, 0]])
y = np.array([1, 1, 2, 3, 2, 3, 3, 1, 2])

lr = 0.25                   #скорость обучения
number_epochs = 1000
w1 = np.random.rand(3, 3)   #веса первого слоя
b1 = np.random.rand(1, 3)   #смещения первого слоя 

w2 = np.random.rand(3, 3)   #веса второго слоя
b2 = np.random.rand(1, 3)   #значения второго слоя 

for epoch in range(number_epochs):
  t1 = x @ w1 + b1            #промежуточные значения (t - temp)
  h1 =sigm(t1)                #значения функции активации от t1

  tout = h1 @ w2 + b2       
  hout = sigm(tout)

  y_full = y_to_vector(y)                             #выходные значения

  #Вычисление градиента
  de_dtout = hout - y_full                            #ошибка последнего слоя
  de_dwout = h1.T @ de_dtout  
  de_dbout = np.sum(de_dtout, axis=0, keepdims=True)  #смещения для выходного слоя

  de_dh1 = de_dtout @ w2.T    
  de_dt1 = de_dh1 * sigm_der(t1)
  de_dw1 = x.T @ de_dt1
  de_db1 = np.sum(de_dt1, axis=0, keepdims=True)      #смещения для скрытого слоя

  #Градиентный спуск
  w1 = w1 - lr * de_dw1
  b1 = b1 - lr * de_db1
  w2 = w2 - lr * de_dwout
  b2 = b2 - lr * de_dbout

  
  counter = 0
  for i in range(len(x)): 
    t1 = x[i] @ w1 + b1
    h1 = sigm(t1)

    tout = h1 @ w2 + b2
    hout = sigm(tout)
    if np.argmax(hout) + 1 == y[i]: counter += 1      #подсчет верных выводов
  if (epoch + 1) % 100 == 0: print('Точность на эпохе', (epoch + 1), ' = ', counter / 9)
  if counter / 9 == 1.0: 
    print('Точность на эпохе', (epoch + 1), ' = ', counter / 9)
    break
